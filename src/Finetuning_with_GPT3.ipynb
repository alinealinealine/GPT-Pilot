{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alinealinealine/GPT-Pilot/blob/main/src/Finetuning_with_GPT3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tune GPT-3 for AIMM narrative ex-ante\n",
        "\n",
        "OpenAI's GPT-3 is a natural language model trained on large set of training data. It can be used for various tasks, including to generate data. \n",
        "\n",
        "However, the model is generalist in nature and thus, not fit for specialised tasks in its original or vaniall version. However, will a bit of finetuning it can be used for more specialised tasks such as generating AIMM text. \n",
        "\n",
        "The fine-tuning happens via OpenAI's API to fine tune GPT-3. "
      ],
      "metadata": {
        "id": "IKvXNTD4z6hW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing dependencies and libraries"
      ],
      "metadata": {
        "id": "Ok-vp7KIzqtR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sm4C8Csy9Uo",
        "outputId": "57b4a824-db96-4a1f-fb88-bc276df50da4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                        | 10 kB 19.5 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                 | 20 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà          | 30 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 40 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44 kB 1.8 MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.9 MB 9.5 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 146 kB 68.8 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 182 kB 59.7 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 168 kB 56.7 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 62 kB 1.4 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 168 kB 39.0 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 166 kB 63.1 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 166 kB 65.4 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 162 kB 64.9 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 162 kB 61.7 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 158 kB 65.6 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 60.5 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 62.7 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 64.7 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 64.1 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 77.1 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 69.8 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 63.3 MB/s \n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 156 kB 64.7 MB/s \n",
            "\u001b[?25h  Building wheel for openai (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -Uq openai wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import wandb\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "x48qUFw30-NU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Entering API Credentials\n",
        "openai.api_key_path = \"./api.txt\""
      ],
      "metadata": {
        "id": "b87W6_B11Wvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Prepration\n",
        "\n",
        "The dataset was processed in R by scrapping relevant documents and cleaning it into the JSON format required to finetune the mdoel. The dataset are split based on sector and portion of AIMM narrative it is expected to generate. \n",
        "\n",
        "1. Sector:\n",
        "  1. FIG\n",
        "  2. MAS\n",
        "  3. CDF\n",
        "  4. INR\n",
        "2. Section of AIMM narrative\n",
        "  1. Project narrative\n",
        "  2. Market narrative \n",
        "  3. Indicators\n",
        "\n",
        "In addition different variation of prompts are also explored - creating different models. \n",
        "\n",
        "## Model naming convention\n",
        "\n",
        "In order to keep track of the models they are to be named using the following convention: \"SSS-IN-GEN-XXXX\"\n",
        "* SSS: Refers to Sector of the model's focus\n",
        "* IN: Refers to model input, can be BP for Board Papers and GE for Generic documents\n",
        "* GEN: Refers to which section the model is trying to generate. Can be one of the following:\n",
        "  * PRO: Project narrative\n",
        "  * MAR: Market narrative\n",
        "  * IND: Indicators\n",
        "* XXXX: Refers to the number of the model - as various models might be created to accomodate various prompts. This can also be alpha numeric.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CFHJWRAXoOcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "project_name = 'FIG-BP-PRO'\n",
        "model_name = 'FIG-BP-PRO-0001'"
      ],
      "metadata": {
        "id": "8wBYzA_hCm8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning \n",
        "\n",
        "Fine tuning involves the following steps:\n",
        "1. Preparing the dataset: Datset is here split into training and validation sets. Before the split - the prompts are also shared with OpenAI to see if they are aligned with the requirements for finetuning.\n",
        "2. FineTuning: This is where the split datasets are shared with OpenAI for finetuning of the GPT model. The final model is saved and can be accessed both here and on OpenAI playground.  \n",
        "\n",
        "## Preping the dataset"
      ],
      "metadata": {
        "id": "li0JRCSo6ISg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a job for splitting dataset\n",
        "run = wandb.init(project=project_name, job_type='split dataset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "KiHs1hwP7OqG",
        "outputId": "214b63bc-ca38-4f48-bfb2-7241aa353c1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgauravrpjain\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20221122_050936-2tji4751</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/gauravrpjain/GPT-3/runs/2tji4751\" target=\"_blank\">rosy-paper-2</a></strong> to <a href=\"https://wandb.ai/gauravrpjain/GPT-3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download full dataset\n",
        "dataset_path = \"./dataset_1.json\""
      ],
      "metadata": {
        "id": "ZxedbhgI7Pg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head $dataset_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_nHuJzL8HL7",
        "outputId": "32e932c7-7165-4a52-fe0b-b4aae004d2a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"prompt\":\"IFC has entered into Memorandum of Understanding (MoUs) with i) Kerala Infrastructure Investment Fund Board (KIFB); ii) PPP Department, Government of Goa; and iii) Gujarat Power Corporation Limited (GPCL). \\n\\nIFC will support KIFB and the Government of Goa in identification and screening of Public-Private Partnership (PPP) projects across infrastructure sectors and undertake pre-feasibility assessments of select projects. \\n\\nIFC will also support GPCL to conduct a pre-feasibility assessment for a potential pilot project to produce clean hydrogen-based renewable energy at one of GPCL's sites in Gujarat. \\nDevelopment Impact: \\n\\n###\\n\\n\",\"completion\":\" \\tIdentification of at least one climate friendly PPP transaction based on the screening and pre-feasibility assessments being undertaken with multiple entities\\n\\tMobilization of private sector investment\\n\\tCreation of jobs\\n##\\n\"}\n",
            "{\"prompt\":\"¬†The proposed investment comprises of (i) a 3-year senior unsecured loan of up to US$40 million with an annual amortizing structure to support climate linked trade and (ii) an unfunded guarantee facility under the Global Trade Finance Guarantee Program of IFC (the 'Project') to support trade finance expansion in Bangladesh. The facilities will be extended to Dutch Bangla Bank Limited ('DBBL' or the 'Bank'), a large private bank with an asset base of US$5.3 billion (4th largest amongst conventional private commercial banks) and ranking 2nd in terms of deposits. \\nDevelopment Impact: \\n\\n###\\n\\n\",\"completion\":\" IFC anticipates supporting DBBL through the proposed financing package to strengthen DBBL's existing portfolio of climate investments. This Project is expected to  (i) enable DBBL to refocus its existing industrial exposures to adapting more sustainable operations as it has significant exposures in business segments that require adoption of climate friendly practices to be competitive and operational such as RMG and textiles , and (ii) demonstrating the potential of increasing green financing and supporting the adoption of green practices by working with industry partners in export-oriented industries, a key economic segment in Bangladesh.   IFC's climate linked trade loan is expected to refocus existing and new trade portfolios of Banks in Bangladesh towards adopting climate related and environmentally sustainable practices amongst borrowers.\\n##\\n\"}\n",
            "{\"prompt\":\"¬†¬†The proposed loan of up to 8.3 million (the IFC Loan) to SIAGRO S.A. (the 'Company', the 'Borrower' or 'Kirene'), a still and carbonated soft drinks (CSD) processing company, is to support the Borrower's investment program and includes: (i) expanding production capacity; (ii) Green building standards and certification, and (iv) working capital requirements.The project¬†is expected to be supported¬†by¬†the Canada-IFC Blended Climate Finance Program (BCFP), as described in the Blended Finance Section. \\nDevelopment Impact: \\n\\n###\\n\\n\",\"completion\":\" The expected impacts are likely to center around:\\nStrengthen downstream linkages: Kirene distributes its products through about  SME wholesalers and\\nsemi-wholesalers in Senegal, which it actively supports with its own sales team and promotional /\\nmarketing materials. \\nEconomy-wide effects: By increasing production capacity, the Project will create direct and indirect jobs,\\nas well as incremental value added.\\nConsumers: Consumption of sugar-dense beverages (both CSD and juices) is associated with the\\nnegative nutritional outcomes for the consumers. The impacts could be mitigated by exploring reformulation\\nor shifting production towards healthier choices.\\nEnvironment: The project will promote recycling and, hence reduce the related carbon footprint and plastic waste. Kirene will promote recycling, hence reducing the related carbon footprint and waste. There may\\nbe scope for market impacts along this dimension in the FMCG market in Senegal.\\n##\\n\"}\n",
            "{\"prompt\":\"The proposed project (the 'Project') consists of an unsecured senior loan of up to XAF3 billion ('Central African Franc') or approximately US$4.6 million equivalent to La R√©gionale (the 'Company' or the 'MFI'). The Project will provide local currency funding to support the microfinance institution ('MFI') in increasing its lending capacity to the retail segment, including Micro, Small and Medium Enterprises ('MSMEs') in Cameroon. The project will be processed under IFC's COVID Base of Pyramid ('BOP') Facility and will be supported by IDA-PSW Blended Finance Facility ('IDA PSW-BFF') and IDA-PSW Local Currency Facility ('IDA PSW-LCF'). \\nDevelopment Impact: \\n\\n###\\n\\n\",\"completion\":\" The most significant expected project-level outcome for the project is access to finance for SMEs to support economic activity and resilience in the aftermath of the COVID-19 crisis. Beyond the project outcome, IFC anticipates that the Project will support resilience amongst base-of-pyramid (BoP) finance providers demonstrating the viability of lending to BoP providers and crowding in other sources of international and domestic finance in support of these lenders.\\n##\\n\"}\n",
            "{\"prompt\":\"¬†ALD S.A. (ALD), a subsidiary of Soci√©t√© G√©n√©rale, is a global fleet management and long-term vehicle leasing service provider operating a fleet of above 1.7 million vehicles with operations in over 40 countries. IFC is proposing to invest up to US$400 million in the form of senior loans to support ALD in financing the green transition of its fleet across emerging market subsidiaries.¬†¬†¬†¬† \\nDevelopment Impact: \\n\\n###\\n\\n\",\"completion\":\" i.) Significantly increase the number of green vehicles (GVs) in the targeted 8 emerging market (EM) countries, and thereby also increase the demand for electric vehicle infrastructure in these markets. ii.) Due to the environmental benefits of GVs, the Project is expected to contribute to a reduction in greenhouse gas (GHG) emissions in the 8 target EMs. iii.) The project will help prove the financial viability of GV leasing by demonstrating the business case of GV's lower total cost of ownership as compared to vehicles with internal combustion engines.\\n##\\n\"}\n",
            "{\"prompt\":\"The project consists of a Working Capital Solutions ('WCS') facility to Cr√©dit du Maroc ('CdM' or the 'Bank') of up to US$50 million. The facility will have a maturity of up to 3 years. The project will enable the Bank to expand its foreign currency trade and working capital lending to enterprises in Morocco, including to small and medium-sized enterprises ('SMEs'). \\nDevelopment Impact: \\n\\n###\\n\\n\",\"completion\":\" The most significant project outcome will be the access to working capital financing for Cr√©dit du Maroc and through Cr√©dit du Maroc, its customers. The IFC facility will support Cr√©dit du Maroc's capacity to sustain its customers' operations in a challenging macroeconomic context. The project is also expected to help preserve Morocco's private sector by supplying foreign currency working capital to support operations of businesses in this context, thus making it more resilient.\\n##\\n\"}\n",
            "{\"prompt\":\"¬†Fundaci√≥n Educacional Santo Tom√°s ('Santo Tom√°s' or the 'Company') is a non-for-profit higher education private institution in Chile. Santo Tom√°s has over 89,000 students in 56 campuses spread over 19 cities across Chile including Arica, Iquique, Antofagasta, Copiap√≥, La Serena, Ovalle, Vi√±a Del Mar, Santiago, Rancagua, Curic√≥, Talca, Chillan, Concepci√≥n, Los Angeles, Temuco, Valdivia, Osorno, Puerto Montt and Punta Arenas. Santo Tom√°s targets lower and emerging middle-income segments operating under three different units which were founded from 1982:¬†Universidad Santo Tom√°s (UST):¬† Universities with 13 campuses across Chile and serves 27.9k students (31% of Santo Tom√°s' total).¬†Instituto Profesional Santo Tom√°s (IPST): A professional institute with 21 campuses across Chile and serves 24.0k students (27% of Santo Tom√°s' total).¬†Centro de Formaci√≥n T√©cnica Santo Tom√°s ('CFTST'): A technical education center with 22 campuses across Chile serving 37.3k students (42% of Santo Tom√°s' total).¬† \\nDevelopment Impact: \\n\\n###\\n\\n\",\"completion\":\" The development impact of the project is linked to an increase in students reached and benefits regarding the student's employability. Additionally, IFC aims to support Santo Tom√°s via the implementation of a Vitae employability 'implementation support' module. Through the development of new infrastructure in combination with the employability component, the Project has the potential to have a market impact through competition, demonstration, and replication channels.\\n##\\n\"}\n",
            "{\"prompt\":\"The proposed investment (the 'Project', or the 'Loan') is a 3-year senior USD-denominated loan of (i) up to US$120 million A Loan on IFC's own account and (ii) Parallel loan(s) of up to US$50 million including a green-shoe option, on best effort basis, to Saigon-Hanoi Commercial Joint Stock Bank ('SHB', or the 'Bank'). The Loan will be used to support the growth of SHB's small and medium enterprises ('SMEs') loan portfolio, including Women-owned SMEs ('WSMEs') and SMEs participating in supply chain finance ('SCF') program. \\nDevelopment Impact: \\n\\n###\\n\\n\",\"completion\":\" The development impact of the Project will be delivered through (i) increased access to finance for SMEs including WSMEs, (ii) increased access to finance for SMEs participating in the SCF financing, and (iii) strengthened institutional readiness and capacity of local banks to offer SCF solutions and improved financial connectivity and integration of SMEs in domestic and global value chains and improved access to formal finance at more favorable terms.\\n##\\n\"}\n",
            "{\"prompt\":\"¬†The proposed investment is for up to US$50 million, capped at 20% of total commitments, plus US$30m in a co-investment envelope, into Vinci Capital Partners IV, L.P. ('VCP IV' or the 'Fund'), a growth equity fund targeting US$1B in commitments to make mid-market investments in Brazil. The Fund is managed by Vinci Capital Gestora de Recursos Ltda. ('Vinci' or the 'Fund Manager'), a strong partner to CDF with differentiated strategy in the Brazilian mid-cap space. VCP IV is part of Vinci's flagship private equity strategy and is the successor fund to several similar strategies pursued by Vinci since 2003. The Fund will seek to make 8-12 investments in growth stage Brazilian companies, providing equity ticket sizes between US$70 million and US$90 million mostly control and control-oriented investments (50%- 80%) while opportunistically considering minority growth investments (30%- 50%) with strong control rights. VCP IV will be sector agnostic but will focus on industries where Vinci has developed expertise, namely: Healthcare, Consumer & Retail, Education, Financial Services, Technology, and Agribusiness. Within these sectors, Vinci will pursue investments across 5 key Brazil trends they identified: Digitalization, aging population, health & wellness, sustainability, and convenience seeking consumer base.¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† \\nDevelopment Impact: \\n\\n###\\n\\n\",\"completion\":\" Vinci will support value creation for its portfolio in Brazil and commit to investing in at least 25 percent female led portfolio companies for the first time. The market creation claim is to strengthen the competitiveness of Brazil's PE market. This fund will build on Vinci III's early success and supports other recent IFC PE investments in Brazil's to continue to demonstrate the mid-market segment is attractive to institutional investors, reducing the segments reliance of DFIs.\\n##\\n\"}\n",
            "{\"prompt\":\"The he proposed IFC investment consists of the financing of a desalinization plant and water pipeline to provide 1,000 liters per second (l/s) of desalinated water to diverse users in the central V and Metropolitan regions of Chile (the 'Project'). A large portion of the water will be sold under contract to industrial-scale users, while the Project will also sell water to six rural potable water associations (APR, for its Spanish acronym). Aguas Pac√≠fico Holding SpA, a portfolio company of Patria Infrastructure Fund III, an existing IFC client, will own and operate the Project. The Project costs are estimated at US$[822] million, to be financed with long-term debt and equity. The proposed IFC investment consists of up to US$[200] million A Loan and up to US$[459] million B Loan. The Project captures three major components: (1) The Aconcagua Desalination Plant in the bay of Quintero (the 'Desalination Plant'), comprised of a seawater intake submerged pipeline, a brine emissary submerged pipeline, a 1,000 l/s reverse osmosis desalination plant, a 110 kV transmission line connecting the desalination plant (1 km), and a 28 km buried water pipeline between the plant and San Isidro (near Quillota); (2) the San Isidro  Quilapilun Aqueduct (the 'Water Pipeline'), comprised of a pumping station, a 23 kV transmission line connected to the pumping station (5 km), and a 76 km buried water pipeline including a 6 km tunnel between San Isidro and Quilapilun), and distribution tanks in La Dormida, Til Til and Quilapilun, and (3) the Puchuncavi Substation (the 'Substation'), comprised of a 110/23 kV electrical substation at the Desalination Plant site.¬† The financing will consist of two packages: (i) senior secured loans of up to US$[268] million for the Desalination Plant and for the Substation ('Financing Package 1'), and (ii) senior secured loans of up to US$[390] million for the Water Pipeline ('Financing Package 2').¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬† \\nDevelopment Impact: \\n\\n###\\n\\n\",\"completion\":\" The Project's direct impact will be the sustainable supply of water for industrial use in a water-stressed region of Chile. The Valpara√≠so Region has suffered from a 13-year drought that has afflicted the country.\\nBeyond the direct impact, the Project will contribute to market creation as it will be Chile's first multi-use desalination plant. The Project has the potential to demonstrate the viability of this model, encouraging the\\ndevelopment of additional multi-user desalination plants that efficiently address the climate-induced water scarcity in Chile.\\n##\\n\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!openai tools fine_tunes.prepare_data -f $dataset_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcbedLhr8LNz",
        "outputId": "df3e8486-d19c-415c-a832-c18ae3985952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing...\n",
            "\n",
            "- Your file contains 5189 prompt-completion pairs\n",
            "- There are 2 examples that are very long. These are rows: [4562, 4803]\n",
            "For conditional generation, and for classification the examples shouldn't be longer than 2048 tokens.\n",
            "- All prompts end with suffix ` \\nDevelopment Impact: \\n\\n###\\n\\n`. This suffix seems very long. Consider replacing with a shorter suffix, such as `\\n\\n===\\n\\n`\n",
            "- All completions end with suffix `\\n##\\n`\n",
            "\n",
            "Based on the analysis we will perform the following actions:\n",
            "- [Recommended] Remove 2 long examples [Y/n]: Y\n",
            "\n",
            "\n",
            "Your data will be written to a new JSONL file. Proceed [Y/n]: Y\n",
            "\n",
            "Wrote modified file to `./dataset_1_prepared.jsonl`\n",
            "Feel free to take a look!\n",
            "\n",
            "Now use that file when fine-tuning:\n",
            "> openai api fine_tunes.create -t \"./dataset_1_prepared.jsonl\"\n",
            "\n",
            "After you‚Äôve fine-tuned a model, remember that your prompt has to end with the indicator string ` \\nDevelopment Impact: \\n\\n###\\n\\n` for the model to start generating completions, rather than continuing with the prompt. Make sure to include `stop=[\"\\n##\\n\"]` so that the generated texts ends at the expected place.\n",
            "Once your model starts training, it'll approximately take 3.71 hours to train a `curie` model, and less for `ada` and `babbage`. Queue will approximately take half an hour per job ahead of you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"./dataset_1_prepared.jsonl\"\n",
        "# check number of samples\n",
        "!wc -l $dataset_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ti7EhJ-r8R-D",
        "outputId": "5cc36a11-b821-4f12-b37b-16e06edfb055"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5187 ./dataset_1_prepared.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting it into training and testing set randomly with 25% going to testing set. \n",
        "* Training Set = 75% \n",
        "* Validation Set = 25% \n",
        "\n",
        "Also, logging the files into W&B for recordkeeping. "
      ],
      "metadata": {
        "id": "lb_lbts_pIDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_json(dataset_path, orient='records', lines=True)\n",
        "df_train, df_test = train_test_split(df,test_size = 0.25,random_state = 42, shuffle = False)\n",
        "df_train.to_json(\"./dataset_1_train.jsonl\", orient='records', lines=True)\n",
        "df_test.to_json(\"./dataset_1_test.jsonl\", orient='records', lines=True)\n",
        "\n",
        "#Logging the files and tables into W&B \n",
        "table_train = wandb.Table(dataframe=df_train)\n",
        "table_valid = wandb.Table(dataframe=df_test)\n",
        "\n",
        "# Create artifacts\n",
        "artifact_train = wandb.Artifact('dataset_1_train.jsonl', type='training_files', metadata={'samples': df_train.shape[0]})\n",
        "artifact_train.add_file('dataset_1_train.jsonl')\n",
        "artifact_train.add(table_train, 'df_1_train')\n",
        "\n",
        "artifact_valid = wandb.Artifact('dataset_1_test.jsonl', type='validation_files', metadata={'samples': df_test.shape[0]})\n",
        "artifact_valid.add_file('dataset_1_test.jsonl')\n",
        "artifact_valid.add(table_valid, 'df_1_test')\n",
        "\n",
        "# Log files\n",
        "run.log_artifact(artifact_train)\n",
        "run.log_artifact(artifact_valid)"
      ],
      "metadata": {
        "id": "58mTu1PG8UfL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5675eb78-7729-4410-dbe3-8475d3b78a83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_artifacts.Artifact at 0x7f8df387ad10>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Closing our dataprep run"
      ],
      "metadata": {
        "id": "wL8pKbh0qqXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# keep entity for reference of artifact later \n",
        "entity = wandb.run.entity\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "vkXaYc40qsKf",
        "outputId": "1db8a80e-46de-4835-8e9a-8a526f23065c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">rosy-paper-2</strong>: <a href=\"https://wandb.ai/gauravrpjain/GPT-3/runs/2tji4751\" target=\"_blank\">https://wandb.ai/gauravrpjain/GPT-3/runs/2tji4751</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20221122_050936-2tji4751/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tuning the model\n"
      ],
      "metadata": {
        "id": "20f5YU5erAgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_file = \"./dataset_1_train.jsonl\"\n",
        "valid_file = \"./dataset_1_test.jsonl\""
      ],
      "metadata": {
        "id": "A51VozQnsD5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining Hyper parameters: \n",
        "\n",
        "Using the default hyper parameters by OpenAI, replacing model with Divinci 003."
      ],
      "metadata": {
        "id": "AytRIGhisVq8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining hyper parameters (using the default ones)\n",
        "model = 'curie'  # using the cheapest model : ada\n",
        "n_epochs = 4\n",
        "batch_size = 4\n",
        "learning_rate_multiplier = 0.1\n",
        "prompt_loss_weight = 0.1"
      ],
      "metadata": {
        "id": "9zaCtwX_r2IQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<API KEY>\""
      ],
      "metadata": {
        "id": "E2yw5YFUxW1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api fine_tunes.create \\\n",
        "    -t $train_file \\\n",
        "    -v $valid_file \\\n",
        "    -m $model \\\n",
        "    --n_epochs $n_epochs \\\n",
        "    --batch_size $batch_size \\\n",
        "    --learning_rate_multiplier $learning_rate_multiplier \\\n",
        "    --prompt_loss_weight $prompt_loss_weight\n",
        "    --suffix $model_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5etp9msr8q4",
        "outputId": "e98bb862-f44e-4e7c-9ee6-dbe6302d1056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rUpload progress:   0% 0.00/5.99M [00:00<?, ?it/s]\rUpload progress: 100% 5.99M/5.99M [00:00<00:00, 7.89Git/s]\n",
            "Uploaded file from ./dataset_1_train.jsonl: file-xfIlHxiG7BRHO6uQmvzbz47r\n",
            "Upload progress: 100% 2.40M/2.40M [00:00<00:00, 3.04Git/s]\n",
            "Uploaded file from ./dataset_1_test.jsonl: file-njPKfzbNv5hG1mS5AQxIe4hY\n",
            "Created fine-tune: ft-fNi5iGbSjUvAhWw9Mw3BIAiB\n",
            "Streaming events until fine-tuning is complete...\n",
            "\n",
            "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
            "[2022-11-22 05:21:31] Created fine-tune: ft-fNi5iGbSjUvAhWw9Mw3BIAiB\n",
            "[2022-11-22 05:21:44] Fine-tune costs $1.91\n",
            "[2022-11-22 05:21:44] Fine-tune enqueued. Queue number: 0\n",
            "[2022-11-22 05:21:46] Fine-tune started\n",
            "\n",
            "Stream interrupted (client disconnected).\n",
            "To resume the stream, run:\n",
            "\n",
            "  openai api fine_tunes.follow -i ft-fNi5iGbSjUvAhWw9Mw3BIAiB\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!openai api fine_tunes.follow -i ft-fNi5iGbSjUvAhWw9Mw3BIAiB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMfui19k0vkt",
        "outputId": "25ddef60-25cf-4b84-d719-919cc06c05fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2022-11-22 05:21:31] Created fine-tune: ft-fNi5iGbSjUvAhWw9Mw3BIAiB\n",
            "[2022-11-22 05:21:44] Fine-tune costs $1.91\n",
            "[2022-11-22 05:21:44] Fine-tune enqueued. Queue number: 0\n",
            "[2022-11-22 05:21:46] Fine-tune started\n",
            "[2022-11-22 05:27:54] Completed epoch 1/4\n",
            "[2022-11-22 05:33:43] Completed epoch 2/4\n",
            "[2022-11-22 05:39:33] Completed epoch 3/4\n",
            "[2022-11-22 05:45:22] Completed epoch 4/4\n",
            "[2022-11-22 05:45:40] Uploaded model: ada:ft-personal-2022-11-22-05-45-38\n",
            "[2022-11-22 05:45:42] Uploaded result file: file-etqWtnfTghm7ZULNBkNSBzkV\n",
            "[2022-11-22 05:45:42] Fine-tune succeeded\n",
            "\n",
            "Job complete! Status: succeeded üéâ\n",
            "Try out your fine-tuned model:\n",
            "\n",
            "openai api completions.create -m ada:ft-personal-2022-11-22-05-45-38 -p <YOUR_PROMPT>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Syncing FineTune Jobs to W&B\n",
        " \n",
        " Logging Fine Tune with W&B to use later\n",
        " "
      ],
      "metadata": {
        "id": "ZkzTIoNgsjxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!openai wandb sync\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuZjW0Jqsro1",
        "outputId": "a57a98a5-a154-40d6-becd-5c2f546f2510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgauravrpjain\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20221122_055012-ft-fNi5iGbSjUvAhWw9Mw3BIAiB\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mft-fNi5iGbSjUvAhWw9Mw3BIAiB\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/gauravrpjain/GPT-3\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/gauravrpjain/GPT-3/runs/ft-fNi5iGbSjUvAhWw9Mw3BIAiB\u001b[0m\n",
            "File file-xfIlHxiG7BRHO6uQmvzbz47r could not be retrieved. Make sure you are allowed to download training/validation files\n",
            "File file-njPKfzbNv5hG1mS5AQxIe4hY could not be retrieved. Make sure you are allowed to download training/validation files\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             elapsed_examples ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               elapsed_tokens ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                training_loss ‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñÑ‚ñá‚ñà‚ñá‚ñÜ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñá‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÅ‚ñÖ‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÅ‚ñÑ‚ñÑ‚ñÑ‚ñÇ‚ñÖ‚ñÅ‚ñÑ‚ñá‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   training_sequence_accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      training_token_accuracy ‚ñÉ‚ñÅ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñà‚ñÉ‚ñÉ‚ñÖ‚ñà‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñá‚ñÉ‚ñÉ‚ñÜ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              validation_loss ‚ñÜ‚ñÑ‚ñÜ‚ñá‚ñÉ‚ñÖ‚ñÇ‚ñÇ‚ñÜ‚ñÉ‚ñÖ‚ñÜ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÜ‚ñÉ‚ñÑ‚ñÜ‚ñÉ‚ñà‚ñÑ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÅ‚ñÉ‚ñÑ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: validation_sequence_accuracy ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    validation_token_accuracy ‚ñÑ‚ñÜ‚ñÉ‚ñÇ‚ñÑ‚ñÉ‚ñÇ‚ñÜ‚ñÇ‚ñÉ‚ñÖ‚ñÇ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÇ‚ñÜ‚ñÑ‚ñÅ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÑ‚ñÑ‚ñÇ‚ñÉ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÑ‚ñÅ‚ñÇ‚ñà‚ñÜ‚ñÇ\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             elapsed_examples 15568.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               elapsed_tokens 7509168.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             fine_tuned_model ada:ft-personal-2022...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       status succeeded\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                training_loss 0.8398\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   training_sequence_accuracy 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      training_token_accuracy 0.509\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              validation_loss 1.08016\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: validation_sequence_accuracy 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    validation_token_accuracy 0.39728\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mft-fNi5iGbSjUvAhWw9Mw3BIAiB\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/gauravrpjain/GPT-3/runs/ft-fNi5iGbSjUvAhWw9Mw3BIAiB\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20221122_055012-ft-fNi5iGbSjUvAhWw9Mw3BIAiB/logs\u001b[0m\n",
            "üéâ wandb sync completed successfully\n"
          ]
        }
      ]
    }
  ]
}